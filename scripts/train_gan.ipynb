{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 16:03:28.184357: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-01 16:03:28.195813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-01 16:03:28.206711: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-01 16:03:28.209866: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-01 16:03:28.219133: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-01 16:03:29.235272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "data_loc = os.path.abspath(\"/home1/smaruj/ExpressionGAN/scripts/data\")\n",
    "log_dir = os.path.abspath(\"./logs\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = False\n",
    "# This flag determines whether to generate random data instead of using a dataset from data_loc.\n",
    "# Default: False, meaning the script will use a real dataset unless overridden by this flag.\n",
    "\n",
    "# data_loc\n",
    "data_start = 0\n",
    "# log_dir\n",
    "log_name = \"gan_unbalanced\"\n",
    "# checkpoint = \n",
    "\n",
    "# model_type = \"resnet\"\n",
    "model_type = \"mlp\"\n",
    "# NOT CHECKED FOR RESNET SO FAR!\n",
    "# Determines the type of model architecture, with options for \"resnet\" or \"mlp\" (multi-layer perceptron).\n",
    "\n",
    "train_iters = 10 #500000\n",
    "disc_iters = 5\n",
    "checkpoint_iters = 100\n",
    "latent_dim = 100\n",
    "# Sets the size of the latent space (random noise vector) that the generator will use as input.\n",
    "\n",
    "gen_dim = 100\n",
    "disc_dim = 100\n",
    "gen_layers = 5\n",
    "disc_layers = 5\n",
    "\n",
    "batch_size = 64\n",
    "max_seq_len = 50\n",
    "# Defines the maximum length of DNA sequences in the dataset.\n",
    "\n",
    "vocab = \"dna\"\n",
    "vocab_order = None\n",
    "# Optionally sets a specific order for the one-hot encoding of vocabulary characters.\n",
    "\n",
    "annotate = False\n",
    "validate = False\n",
    "# Determines whether a validation set will be used during training.\n",
    "\n",
    "balanced_bins = False\n",
    "learning_rate = 1e-5\n",
    "lmbda = 10.\n",
    "seed = 42\n",
    "# Sets a random seed for reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% set RNG\n",
    "seed = seed\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% fix vocabulary of model\n",
    "charmap, rev_charmap = lib.dna.get_vocab(vocab, vocab_order)\n",
    "vocab_size = len(charmap)\n",
    "# This function call sets up the vocabulary based on the type of sequence being used (as defined by args.vocab and args.vocab_order).\n",
    "# args.vocab: Specifies the type of vocabulary to use, such as \"dna\" or \"rna\".\n",
    "# args.vocab_order: Optionally provides a custom order for the one-hot encoding of characters.\n",
    "# charmap: A dictionary that maps characters (like 'A', 'T', 'G', 'C' for DNA) to their corresponding one-hot encoding indices.\n",
    "# rev_charmap: The reverse mapping from one-hot encoded indices back to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(vocab_size)\n",
    "# This creates an identity matrix of size vocab_size. The identity matrix will be used for one-hot encoding of the sequences, \n",
    "# where each character (nucleotide) is represented by a unique row in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% organize model logs/checkpoints\n",
    "#logdir, checkpoint_baseline = lib.log(args, samples_dir=True)\n",
    "# This line calls a function lib.log() to set up directories for saving logs and model checkpoints during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 16:03:40.810027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43626 MB memory:  -> device: 0, name: NVIDIA L40S, pci bus id: 0000:42:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "#%% build GAN\n",
    "latent_vars = tf.Variable(tf.random.normal(shape=[batch_size, latent_dim], seed=seed), name='latent_vars')\n",
    "# This line initializes the latent space variables (latent_vars) that the generator will take as input. \n",
    "# These variables are sampled from a normal distribution and are of shape [args.batch_size, args.latent_dim].\n",
    "data_enc_dim = vocab_size + 1 if annotate else vocab_size\n",
    "data_size = max_seq_len * data_enc_dim\n",
    "# The data encoding dimension data_enc_dim is adjusted based on whether annotations are included or not.\n",
    "# If args.annotate is True, the encoding will have an additional annotation channel (hence vocab_size + 1).\n",
    "# Otherwise, the encoding will only include the vocabulary size (vocab_size).\n",
    "# data_size: Total size of the encoded sequence data, calculated as the maximum sequence length (args.max_seq_len) \n",
    "# multiplied by the encoding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.variable_scope(\"Generator\", reuse=None) as scope:\n",
    "  if model_type==\"mlp\":\n",
    "    generator_model = lib.models.mlp_generator((latent_dim,), dim=gen_dim, output_size=50, num_layers=gen_layers)\n",
    "    gen_data = generator_model(latent_vars)\n",
    "    # changed the output_size to match the length of sequence\n",
    "  elif model_type==\"resnet\":\n",
    "    gen_data = lib.models.resnet_generator(latent_vars, gen_dim, max_seq_len, data_enc_dim, annotate)\n",
    "#  gen_vars = lib.get_vars(scope)\n",
    "# This block defines the Generator network, which takes the latent variables and generates DNA sequences:\n",
    "# The type of model (mlp or resnet) is selected based on args.model_type.\n",
    "# MLP Generator: A fully connected multilayer perceptron (MLP) that maps latent space to the DNA sequence.\n",
    "# ResNet Generator: A ResNet-based architecture that generates more complex structures.\n",
    "# gen_vars: The trainable variables of the generator (used later for optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"mlp\":\n",
    "    real_data = tf.random.normal([batch_size, max_seq_len])  # Will be replaces real data source\n",
    "    eps = tf.random.uniform([batch_size, 1])  # Use tf.random.uniform instead of tf.random_uniform\n",
    "elif model_type == \"resnet\":\n",
    "    real_data = tf.random.normal([batch_size, max_seq_len, data_enc_dim])  # Will be replaces real data source\n",
    "    eps = tf.random.uniform([batch_size, 1, 1])  # Use tf.random.uniform\n",
    "\n",
    "# Interpolation between real_data and gen_data\n",
    "interp = eps * real_data + (1 - eps) * gen_data\n",
    "# This block sets up placeholders for real data (real_data) and calculates an interpolated data point between real and \n",
    "# generated data (used for gradient penalty, in case this is a Wasserstein GAN with gradient penalty (WGAN-GP)):\n",
    "# real_data: Placeholder for real DNA sequences, shaped differently based on whether it's an MLP or ResNet model.\n",
    "# eps: A random variable used to linearly interpolate between real and generated data.\n",
    "# interp: The interpolation between real and generated data, used for the Lipschitz continuity constraint in WGAN-GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 50), dtype=float32, numpy=\n",
       "array([[ 0.22526273, -0.5796349 ,  0.28014633, ...,  0.57496464,\n",
       "         0.4842909 , -0.22990184],\n",
       "       [ 0.10483949, -0.47229636, -0.0832711 , ...,  0.1461472 ,\n",
       "        -0.75750464, -0.75956464],\n",
       "       [-0.11446839, -1.3069288 , -0.7515979 , ...,  0.9511246 ,\n",
       "        -0.06501768,  0.29098797],\n",
       "       ...,\n",
       "       [-0.08224764,  0.13212356, -0.6718516 , ...,  0.80016863,\n",
       "         0.21342859, -0.91508466],\n",
       "       [ 0.2805145 , -0.05317048,  0.00689527, ..., -1.0051547 ,\n",
       "        -0.7431211 , -0.38497156],\n",
       "       [ 0.04917089, -0.06297167,  0.84044915, ...,  0.14386684,\n",
       "         0.0486706 ,  0.3304389 ]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.variable_scope(\"Discriminator\", reuse=None) as scope:\n",
    "  if model_type==\"mlp\":\n",
    "    discriminator_model = lib.models.mlp_discriminator((50,), dim=disc_dim, num_layers=disc_layers)\n",
    "    gen_score = discriminator_model(gen_data)\n",
    "    # changed frm dara_size to 50\n",
    "  elif model_type==\"resnet\":\n",
    "    gen_score = lib.models.resnet_discriminator(gen_data, disc_dim, max_seq_len, data_enc_dim, res_layers=disc_layers)\n",
    "#  disc_vars = lib.get_vars(scope)\n",
    "# This block defines the Discriminator network, which scores both real and generated sequences: It checks whether the input is real or fake.\n",
    "# Similar to the generator, the discriminator architecture is chosen based on whether it's an MLP or ResNet.\n",
    "# gen_score: The discriminator’s score for generated data (how “real” it thinks the generated data is).\n",
    "# disc_vars: The trainable variables of the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.variable_scope(\"Discriminator\", reuse=True) as scope:\n",
    "  if model_type==\"mlp\":\n",
    "    real_score = discriminator_model(real_data)\n",
    "    interp_score = discriminator_model(interp)\n",
    "  elif model_type==\"resnet\":\n",
    "    real_score = lib.models.resnet_discriminator(real_data, disc_dim, max_seq_len, data_enc_dim, res_layers=disc_layers)\n",
    "    interp_score = lib.models.resnet_discriminator(interp, disc_dim, max_seq_len, data_enc_dim, res_layers=disc_layers)\n",
    "# This re-uses the discriminator to score both the real and interpolated data:\n",
    "# real_score: The discriminator's score for real data.\n",
    "# interp_score: The discriminator's score for the interpolated data (used for gradient penalty in WGAN-GP).\n",
    "# This block reuses the discriminator with reuse=True so that the same weights are used for real, generated, and interpolated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% cost function\n",
    "mean_gen_score = tf.reduce_mean(gen_score)\n",
    "mean_real_score = tf.reduce_mean(real_score)\n",
    "# gen_score and real_score: These are the outputs of the discriminator when given either generated data (gen_score) or \n",
    "# real data (real_score). These scores represent how likely the discriminator thinks the data is \"real\" (with higher values \n",
    "# indicating more \"real\").\n",
    "\n",
    "gen_cost = - mean_gen_score #tf.reduce_mean(gen_score)\n",
    "# The goal of the Generator is to maximize the discriminator's score for the generated data, which means it wants the \n",
    "# generated data to appear as real as possible. By negating mean_gen_score, the generator will be trained to increase this \n",
    "# value (since optimizers minimize the loss).\n",
    "disc_diff = mean_gen_score - mean_real_score \n",
    "# The Discriminator wants to maximize the difference between the scores it gives to real and fake data, i.e., \n",
    "# it wants the real data to have high scores and the generated data to have low scores.\n",
    "# disc_diff measures the gap between the mean score assigned to fake data (mean_gen_score) and real data (mean_real_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line does exactly the same as the line above\n",
    "# disc_diff = tf.reduce_mean(gen_score) - tf.reduce_mean(real_score)\n",
    "#%% gradient penalty\n",
    "# grads = tf.gradients(interp_score, interp)[0]\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass: calculate the output (scores)\n",
    "    tape.watch(interp)\n",
    "\n",
    "    # Get the score from the discriminator for the interpolated data\n",
    "    if model_type == \"mlp\":\n",
    "        interp_score = discriminator_model(interp)\n",
    "    # elif model_type == \"resnet\":\n",
    "    #     interp_score = lib.models.resnet_discriminator(interp, disc_dim, max_seq_len, data_enc_dim, res_layers=disc_layers)\n",
    "    \n",
    "# Compute gradients\n",
    "grads = tape.gradient(interp_score, interp)\n",
    "\n",
    "# interp_score: This is the score from the discriminator for the interpolated data between real and generated samples.\n",
    "# The interpolated data, a combination of real data and generated data, is used to compute the gradients.\n",
    "# tf.gradients(interp_score, interp): This computes the gradients of the discriminator's score with respect to the interpolated samples. The gradients describe how much the discriminator's output changes when the interpolated data changes.\n",
    "# Why interpolate? In WGAN-GP, the gradient penalty is applied to points interpolated between real and generated data \n",
    "# to enforce smooth transitions between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_norms = tf.norm(grads, axis=1) # might need extra term for numerical stability of SGD\n",
    "# This calculates the norm (magnitude) of the gradients along the specified axes (here, over all spatial dimensions of the data). \n",
    "# Essentially, this gives the overall strength of the gradient at each interpolated point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_penalty = lmbda * tf.reduce_mean((grad_norms - 1.) ** 2)\n",
    "# grad_norms - 1.: In a well-behaved discriminator that respects the Lipschitz constraint, the gradients with respect to the input \n",
    "# should have a norm close to 1. This term measures how far the gradient norms are from 1.\n",
    "# (grad_norms - 1.) ** 2: This squares the difference, penalizing gradients that deviate significantly from 1.\n",
    "# tf.reduce_mean((grad_norms - 1.) ** 2): The penalty is averaged over all the interpolated samples to create a single penalty term.\n",
    "# args.lmbda: This is a hyperparameter that controls the strength of the gradient penalty. \n",
    "# It ensures that the discriminator learns a Lipschitz function by penalizing gradient norms that are far from 1.\n",
    "disc_cost = disc_diff + grad_penalty\n",
    "# disc_cost: The total discriminator loss, which now includes two components:\n",
    "# disc_diff: The difference between the discriminator’s score for generated and real data. This encourages the discriminator to distinguish between real and fake data.\n",
    "# grad_penalty: The gradient penalty term, which enforces the Lipschitz constraint by penalizing large deviations from the desired gradient norm of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create the Adam optimizer for the generator\n",
    "gen_optimizer = Adam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.9, name='gen_optimizer')\n",
    "\n",
    "# Create the Adam optimizer for the discriminator\n",
    "disc_optimizer = Adam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.9, name='disc_optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cost = []\n",
    "gen_costs = []\n",
    "gen_scores = []\n",
    "real_scores = []\n",
    "gen_counts = []\n",
    "train_counts = []\n",
    "valid_cost = []\n",
    "valid_counts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def feed(batch_size=batch_size, seq_len=max_seq_len, data_len=None):\n",
    "    while True:\n",
    "        samples = np.random.choice(vocab_size, [batch_size, seq_len])\n",
    "        data = np.vstack([np.expand_dims(I[vec], 0) for vec in samples])\n",
    "        if model_type == \"mlp\":\n",
    "            reshaped_data = np.reshape(data, [batch_size, -1])\n",
    "        elif model_type == \"resnet\":\n",
    "            reshaped_data = data\n",
    "        yield reshaped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading seqs data!\n"
     ]
    }
   ],
   "source": [
    "if generic:\n",
    "    print(\"\\n Inside if and preparing random data!\")\n",
    "    if annotate:\n",
    "        raise Exception(\"args `annotate` and `generic` are incompatible.\")\n",
    "\n",
    "    train_seqs = feed()\n",
    "    if validate:\n",
    "        valid_seqs = feed(data_len=100)\n",
    "else:\n",
    "    print(\"\\n Loading seqs data!\")\n",
    "    data = lib.dna.load(data_loc, vocab_order=vocab_order, max_seq_len=max_seq_len,\n",
    "                         data_start_line=data_start, vocab=vocab, valid=validate,\n",
    "                         annotate=annotate)\n",
    "    if validate:\n",
    "        split = len(data) // 2\n",
    "        train_data = data[:split]\n",
    "        valid_data = data[split:]\n",
    "        if len(train_data) == 1:\n",
    "            train_data = train_data[0]\n",
    "        if len(valid_data) == 1:\n",
    "            valid_data = valid_data[0]\n",
    "    else:\n",
    "        train_data = data\n",
    "    if annotate:\n",
    "        if validate:\n",
    "            valid_data = np.concatenate(valid_data, 2)\n",
    "        train_data = np.concatenate(train_data, 2)\n",
    "\n",
    "    def feed(data, batch_size=batch_size):\n",
    "        num_batches = len(data) // batch_size\n",
    "        if model_type == \"mlp\":\n",
    "            reshaped_data = np.reshape(data, [data.shape[0], -1])\n",
    "        elif model_type == \"resnet\":\n",
    "            reshaped_data = data\n",
    "        while True:\n",
    "            for ctr in range(num_batches):\n",
    "                yield reshaped_data[ctr * batch_size: (ctr + 1) * batch_size]\n",
    "\n",
    "    train_seqs = feed(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint (if any)\n",
    "# if args.checkpoint:\n",
    "#     checkpoint = tf.train.Checkpoint(optimizer=gen_optimizer, generator=gen_data, discriminator=real_data)\n",
    "#     checkpoint.restore(args.checkpoint).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAN\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# Train GAN\n",
    "print(\"Training GAN\")\n",
    "print(\"================================================\")\n",
    "fixed_latents = []\n",
    "nSampleBatches = 10\n",
    "for nBatches in range(nSampleBatches):\n",
    "    fixed_latents.append(np.random.normal(size=[batch_size, latent_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(train_iters):\n",
    "    true_count = idx + 1\n",
    "    # Train discriminator\n",
    "    for _ in range(disc_iters):\n",
    "        real_batch = next(train_seqs)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            true_batch = tf.convert_to_tensor(real_batch)\n",
    "            latent_vars = tf.random.normal(shape=[batch_size, latent_dim])\n",
    "            gen_data = generator_model(latent_vars)\n",
    "            \n",
    "            real_score = discriminator_model(true_batch)\n",
    "            \n",
    "            # Compute costs\n",
    "            mean_gen_score = tf.reduce_mean(gen_score)\n",
    "            mean_real_score = tf.reduce_mean(real_score)\n",
    "            gen_cost = -mean_gen_score\n",
    "            disc_diff = mean_gen_score - mean_real_score\n",
    "\n",
    "            interp = eps * real_data + (1 - eps) * gen_data\n",
    "            interp_score = discriminator_model(interp)\n",
    "            \n",
    "            # Gradient penalty\n",
    "            with tf.GradientTape() as penalty_tape:\n",
    "                # Forward pass: calculate the output (scores)\n",
    "                penalty_tape.watch(interp)\n",
    "\n",
    "                # Get the score from the discriminator for the interpolated data\n",
    "                if model_type == \"mlp\":\n",
    "                    interp_score = lib.models.mlp_discriminator(interp, dim=disc_dim, input_size=50, num_layers=disc_layers)\n",
    "                elif model_type == \"resnet\":\n",
    "                    interp_score = lib.models.resnet_discriminator(interp, disc_dim, max_seq_len, data_enc_dim, res_layers=disc_layers)\n",
    "               \n",
    "            # # Compute gradients\n",
    "            grads = penalty_tape.gradient(interp_score, interp)\n",
    "            grad_norms = tf.norm(grads, axis=1)\n",
    "            grad_penalty = lmbda * tf.reduce_mean((grad_norms - 1.) ** 2)\n",
    "            \n",
    "            disc_cost = disc_diff + grad_penalty\n",
    "\n",
    "        print(gradients, disc_vars)\n",
    "        gradients = tape.gradient(disc_cost, disc_vars)\n",
    "        disc_optimizer.apply_gradients(zip(gradients, disc_vars))\n",
    "\n",
    "    # Train generator\n",
    "    with tf.GradientTape() as tape:\n",
    "        latent_vars = tf.random.normal(shape=[batch_size, latent_dim])\n",
    "        gen_data = lib.models.mlp_generator(latent_vars, dim=gen_dim, input_size=latent_dim, output_size=data_size, num_layers=gen_layers)\n",
    "        gen_score = lib.models.discriminator(gen_data, dim=disc_dim, input_size=data_size, num_layers=disc_layers)\n",
    "        gen_cost = -tf.reduce_mean(gen_score)\n",
    "\n",
    "    gradients = tape.gradient(gen_cost, gen_data.trainable_variables)\n",
    "    gen_optimizer.apply_gradients(zip(gradients, gen_data.trainable_variables))\n",
    "\n",
    "    # Log results\n",
    "    if idx % 10 == 0:\n",
    "        train_cost.append(gen_cost.numpy())\n",
    "        gen_costs.append(gen_cost.numpy())\n",
    "        gen_scores.append(mean_gen_score.numpy())\n",
    "        real_scores.append(mean_real_score.numpy())\n",
    "        train_counts.append(true_count)\n",
    "        print(\"Iteration: {}. Generator Cost: {:.4f}, Discriminator Cost: {:.4f}\".format(idx, gen_cost.numpy(), disc_cost.numpy()))\n",
    "    \n",
    "    # Save checkpoints\n",
    "    # if idx % args.checkpoint_iters == 0:\n",
    "    #     checkpoint.save(file_prefix=checkpoint_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Discriminator Trainable Variables:\", disc_vars)\n",
    "print(\"Generator Trainable Variables:\", gen_data.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, save model\n",
    "tf.saved_model.save(gen_data, os.path.join(logdir, \"final_model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9_tf2.15_np1.23",
   "language": "python",
   "name": "py3.9_tf2.15_np1.23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
